<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://kubevirt.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://kubevirt.io//" rel="alternate" type="text/html" /><updated>2022-07-05T20:52:30+00:00</updated><id>https://kubevirt.io//feed.xml</id><title type="html">KubeVirt.io</title><subtitle>Virtual Machine Management on Kubernetes</subtitle><entry><title type="html">KubeVirt at KubeCon EU 2022</title><link href="https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022.html" rel="alternate" type="text/html" title="KubeVirt at KubeCon EU 2022" /><published>2022-06-28T00:00:00+00:00</published><updated>2022-06-28T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-at-KubeCon-EU-2022.html"><![CDATA[<p>KubeCon EU was in Valencia, Spain this year from May 16-20. For many of the 7000+ physical attendees, it was their first in-person conference in several years. With luck, it was the first of many more to come, as KubeCon is a rare opportunity to learn about, from, and with a rich variety of adopters, communities, and vendors that make up the open source and cloud native ecosystem.</p>

<p>The KubeVirt community presented two sessions, both on Wednesday May 18th:</p>
<ol>
  <li>A Virtual Open Office Hours session, and</li>
  <li>A Maintainer Track session: ‚ÄòIt‚Äôs All for the Users. More Durable, Secure, and Pluggable. KubeVirt v0.53‚Äô</li>
</ol>

<h2 id="virtual-open-office-hours">Virtual Open Office Hours</h2>

<p>This was a 45-minute project virtual session, hosted by the CNCF. This was on the Bevy platform (which will be familiar to KubeVirt Summit attendees from the past two years) and we had five lovely people from the KubeVirt community ready with a variety of demos and presentations and to answer questions from attendees:+
Alice Frosi, Itamar Holder, Miguel Duarte de Mora Barroso, Luboslav Pivarc, and Bartosz Rybacki</p>

<p>This was an opportunity for KubeCon attendees (virtual and physical) to ask questions and discuss any topics, and our presenters covered the following: Introduction to KubeVirt, live migration, Istio integration, and CDI hotplug/resize.
Despite some initial technical issues and improvised changes, this session went really well. We had about ~25 consistent attendees, and we received a good range of Q&amp;A and interaction with the attendees on all topics presented. It was a very solid 45 minutes.
Unfortunately, due to a miscommunication, there is no recording of this session.</p>

<p>A huge thanks to the presenters for their time and collaboration in preparing for this.</p>

<h2 id="maintainer-track">Maintainer Track</h2>

<p>Later that day, on the Maintainer Track, Alice also gave an in-depth breakdown of a whole slew of new KubeVirt features and showed a demo with the KubeVirt Cluster API: deploying Kubernetes on top of Kubernetes.
You can watch <a href="https://youtu.be/L9H0pz5PpKo">the CNCF recording here</a>, and download the <a href="https://kccnceu2022.sched.com/event/ytu1">demo video and slides</a> that are available from the schedule.</p>

<p>There was a healthy amount of questions, both during Q&amp;A and after the talk. The participants were particularly interested to know how to prepare and customize VM disks with KubeVirt, how to run Windows VM, especially combined with GPUs, and how to expose the Kubernetes API service of a deployed cluster to the KubeVirt cluster API provider outside of the KubeVirt VM. There were additional questions on the status of TPM support and VM migration when the hosting node goes down.</p>

<h2 id="thank-you">Thank you!</h2>
<p>Big thanks again to our presenters: Alice Frosi, Itamar Holder, Miguel Duarte de Mora Barroso, Luboslav Pivarc, and Bartosz Rybacki.
And everyone who attended the sessions, listened, and asked great questions.</p>

<h2 id="want-to-see-more-from-kubecon-eu-2022">Want to see more from KubeCon EU 2022?</h2>

<p>If you‚Äôre interested in seeing more photos and recordings from the event:</p>
<ul>
  <li><a href="https://www.flickr.com/photos/143247548@N03/albums/72177720298987342">CNCF‚Äôs Photo album (Flickr) of the event</a>.</li>
  <li><a href="https://www.youtube.com/c/cloudnativefdn">The CNCF video recordings of the sessions on Youtube</a>.</li>
  <li>And <a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/schedule/">the event schedule</a> to help you find sessions.</li>
</ul>]]></content><author><name>Andrew Burden</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><category term="KubeCon" /><summary type="html"><![CDATA[A short report on the two sessions KubeVirt presented at KubeCon EU 2022]]></summary></entry><entry><title type="html">KubeVirt v0.54.0</title><link href="https://kubevirt.io//2022/changelog-v0.54.0.html" rel="alternate" type="text/html" title="KubeVirt v0.54.0" /><published>2022-06-08T00:00:00+00:00</published><updated>2022-06-08T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.54.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.54.0.html"><![CDATA[<h2 id="v0540">v0.54.0</h2>

<p>Released on: Wed Jun 8 14:15:43 2022 +0000</p>

<ul>
  <li>[PR #7757][orenc1] new alert for excessive number of VMI migrations in a period of time.</li>
  <li>[PR #7517][ShellyKa13] Add virtctl Memory Dump command</li>
  <li>[PR #7801][VirrageS] Empty (<code class="language-plaintext highlighter-rouge">nil</code> values) of <code class="language-plaintext highlighter-rouge">Address</code> and <code class="language-plaintext highlighter-rouge">Driver</code> fields in XML will be omitted.</li>
  <li>[PR #7475][raspbeep] Adds the reason of a live-migration failure to a recorded event in case EvictionStrategy is set but live-migration is blocked due to its limitations.</li>
  <li>[PR #7739][fossedihelm] Allow <code class="language-plaintext highlighter-rouge">virtualmachines/migrate</code> subresource to admin/edit users</li>
  <li>[PR #7618][lyarwood] The requirement to define a <code class="language-plaintext highlighter-rouge">Disk</code> or <code class="language-plaintext highlighter-rouge">Filesystem</code> for each <code class="language-plaintext highlighter-rouge">Volume</code> associated with a <code class="language-plaintext highlighter-rouge">VirtualMachine</code> has been removed. Any <code class="language-plaintext highlighter-rouge">Volumes</code> without a <code class="language-plaintext highlighter-rouge">Disk</code> or <code class="language-plaintext highlighter-rouge">Filesystem</code> defined will have a <code class="language-plaintext highlighter-rouge">Disk</code> defined within the <code class="language-plaintext highlighter-rouge">VirtualMachineInstance</code> at runtime.</li>
  <li>[PR #7529][xpivarc] NoReadyVirtController and NoReadyVirtOperator should be properly fired.</li>
  <li>[PR #7465][machadovilaca] Add metrics for migrations and respective phases</li>
  <li>[PR #7592][akalenyu] BugFix: virtctl guestfs incorrectly assumes image name</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.54.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.53.0</title><link href="https://kubevirt.io//2022/changelog-v0.53.0.html" rel="alternate" type="text/html" title="KubeVirt v0.53.0" /><published>2022-05-09T00:00:00+00:00</published><updated>2022-05-09T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.53.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.53.0.html"><![CDATA[<h2 id="v0530">v0.53.0</h2>

<p>Released on: Mon May 9 14:02:20 2022 +0000</p>

<ul>
  <li>[PR #7533][akalenyu] Add several VM snapshot metrics</li>
  <li>[PR #7574][rmohr] Pull in cdi dependencies with minimized transitive dependencies to ease API adoption</li>
  <li>[PR #7318][iholder-redhat] Snapshot restores now support restoring to a target VM different than the source</li>
  <li>[PR #7474][borod108] Added the following metrics for live migration: kubevirt_migrate_vmi_data_processed_bytes, kubevirt_migrate_vmi_data_remaining_bytes, kubevirt_migrate_vmi_dirty_memory_rate_bytes</li>
  <li>[PR #7441][rmohr] Add <code class="language-plaintext highlighter-rouge">virtctl scp</code> to ease copying files from and to VMs and VMIs</li>
  <li>[PR #7265][rthallisey] Support steady-state job types in the load-generator tool</li>
  <li>[PR #7544][fossedihelm] Upgraded go version to 1.17.8</li>
  <li>[PR #7582][acardace] Fix failed reported migrations when actually they were successful.</li>
  <li>[PR #7546][0xFelix] Update virtio-container-disk to virtio-win version 0.1.217-1</li>
  <li>[PR #7530][iholder-redhat] [External Kernel Boot]: Disallow kernel args without providing custom kernel</li>
  <li>[PR #7493][davidvossel] Adds new EvictionStrategy ‚ÄúExternal‚Äù for blocking eviction which is handled by an external controller</li>
  <li>[PR #7563][akalenyu] Switch VolumeSnapshot to v1</li>
  <li>[PR #7406][acardace] Reject <code class="language-plaintext highlighter-rouge">LiveMigrate</code> as a workload-update strategy if the <code class="language-plaintext highlighter-rouge">LiveMigration</code> feature gate is not enabled.</li>
  <li>[PR #7103][jean-edouard] Non-persistent vTPM now supported. Keep in mind that the state of the TPM is wiped after each shutdown. Do not enable Bitlocker!</li>
  <li>[PR #7277][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 8.0.0 and QEMU 6.2.0.</li>
  <li>[PR #7130][Barakmor1] Add field to kubevirtCR to set Prometheus ServiceMonitor object‚Äôs namespace</li>
  <li>[PR #7401][iholder-redhat] virt-api deployment is now scalable - replicas are determined by the number of nodes in the cluster</li>
  <li>[PR #7500][awels] BugFix: Fixed RBAC for admin/edit user to allow virtualmachine/addvolume and removevolume. This allows for persistent disks</li>
  <li>[PR #7328][apoorvajagtap] Don‚Äôt ignore ‚Äìidentity-file when setting ‚Äìlocal-ssh=true on <code class="language-plaintext highlighter-rouge">virtctl ssh</code></li>
  <li>[PR #7469][xpivarc] Users can now enable the NonRoot feature gate instead of NonRootExperimental</li>
  <li>[PR #7451][fossedihelm] Reduce virt-launcher memory usage by splitting monitoring and launcher processes</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.53.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.52.0</title><link href="https://kubevirt.io//2022/changelog-v0.52.0.html" rel="alternate" type="text/html" title="KubeVirt v0.52.0" /><published>2022-04-08T00:00:00+00:00</published><updated>2022-04-08T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.52.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.52.0.html"><![CDATA[<h2 id="v0520">v0.52.0</h2>

<p>Released on: Fri Apr 8 16:17:56 2022 +0000</p>

<ul>
  <li>[PR #7024][fossedihelm] Add an warning message if the client and server virtctl versions are not aligned</li>
  <li>[PR #7486][rmohr] Move stable.txt location to a more appropriate path</li>
  <li>[PR #7372][saschagrunert] Fixed <code class="language-plaintext highlighter-rouge">KubeVirtComponentExceedsRequestedMemory</code> alert complaining about many-to-many matching not allowed.</li>
  <li>[PR #7426][iholder-redhat] Add warning for manually determining core-component replica count in Kubevirt CR</li>
  <li>[PR #7424][maiqueb] Provide interface binding types descriptions, which will be featured in the KubeVirt API.</li>
  <li>[PR #7422][orelmisan] Fixed setting custom guest pciAddress and bootOrder parameter(s) to a list of SR-IOV NICs.</li>
  <li>[PR #7421][rmohr] Fix knowhosts file corruption for virtctl ssh</li>
  <li>[PR #6854][rmohr] Make virtctl ssh work with ssh-rsa+ preauthentication</li>
  <li>[PR #7267][iholder-redhat] Applied migration configurations can now be found in VMI‚Äôs status</li>
  <li>[PR #7321][iholder-redhat] [Migration Policies]: precedence to VMI labels over Namespace labels</li>
  <li>[PR #7326][oshoval] The Ginkgo dependency has been upgraded to v2.1.3 (major version upgrade)</li>
  <li>[PR #7361][SeanKnight] Fixed a bug that prevents virtctl from working with clusters accessed via Rancher authentication proxy, or any other cluster where the server URL contains a path component. (#3760)</li>
  <li>[PR #7255][tyleraharrison] Users are now able to specify <code class="language-plaintext highlighter-rouge">--address [ip_address]</code> when using <code class="language-plaintext highlighter-rouge">virtctl vnc</code> rather than only using 127.0.0.1</li>
  <li>[PR #7275][enp0s3] Add observedGeneration to virt-operator to have a race-free way to detect KubeVirt config rollouts</li>
  <li>[PR #7233][xpivarc] Bug fix: Successfully aborted migrations should be reported now</li>
  <li>[PR #7158][AlonaKaplan] Add masquerade VMs support to single stack IPv6.</li>
  <li>[PR #7227][rmohr] Remove VMI informer from virt-api to improve scaling characteristics of virt-api</li>
  <li>[PR #7288][raspbeep] Users now don‚Äôt need to specify container for <code class="language-plaintext highlighter-rouge">kubectl logs &lt;vmi-pod&gt;</code> and <code class="language-plaintext highlighter-rouge">kubectl exec &lt;vmi-pod&gt;</code>.</li>
  <li>[PR #6709][xpivarc] Workloads will be migrated to nonroot implementation if NonRoot feature gate is set. (Except VirtioFS)</li>
  <li>[PR #7241][lyarwood] Fixed a bug that prevents only a unattend.xml configmap or secret being provided as contents for a sysprep disk. (#7240, @lyarwood)</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.52.0 changes]]></summary></entry><entry><title type="html">Load-balancer for virtual machines on bare metal Kubernetes clusters</title><link href="https://kubevirt.io//2022/Virtual-Machines-with-MetalLB.html" rel="alternate" type="text/html" title="Load-balancer for virtual machines on bare metal Kubernetes clusters" /><published>2022-04-03T00:00:00+00:00</published><updated>2022-04-03T00:00:00+00:00</updated><id>https://kubevirt.io//2022/Virtual-Machines-with-MetalLB</id><content type="html" xml:base="https://kubevirt.io//2022/Virtual-Machines-with-MetalLB.html"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>Over the last year, Kubevirt and MetalLB have shown to be powerful duo in order to support fault-tolerant access to an application on virtual machines through an external IP address. 
As a Cluster administrator using an on-prem cluster without a network load-balancer, now it‚Äôs possible to use MetalLB operator to provide load-balancer capabilities (with Services of type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>) to virtual machines.</p>

<h2 id="metallb">MetalLB</h2>

<p><a href="https://metallb.universe.tf/">MetalLB</a> allows you to create Kubernetes services of type <code class="language-plaintext highlighter-rouge">LoadBalancer</code>, and provides network load-balancer implementation in on-prem clusters that don‚Äôt run on a cloud provider.
MetalLB is responsible for assigning/unassigning an external IP Address to your service, using IPs from pre-configured pools. In order for the external IPs to be announced externally, MetalLB works in 2 modes, Layer 2 and BGP:</p>

<ul>
  <li>
    <p>Layer 2 mode (ARP/NDP):</p>

    <p>This mode - which actually does not implement real load-balancing behavior - provides a failover mechanism where a single node owns the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service, until it fails, triggering another node to be chosen as the service owner. This configuration mode makes the IPs reachable from the local network.<br />
In this method, the MetalLB speaker pod announces the IPs in ARP (for IPv4) and NDP (for IPv6) protocols over the host network. From a network perspective, the node owning the service appears to have multiple IP addresses assigned to a network interface. After traffic is routed to the node, the service proxy sends the traffic to the application pods.</p>
  </li>
  <li>
    <p>BGP mode:</p>

    <p>This mode provides real load-balancing behavior, by establishing BGP peering sessions with the network routers - which advertise the external IPs of the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service, distributing the load over the nodes.</p>
  </li>
</ul>

<p>To read more on MetalLB concepts, implementation and limitations, please read <a href="https://metallb.universe.tf/concepts/">its documentation</a>.</p>

<h2 id="demo-virtual-machine-with-external-ip-and-metallb-load-balancer">Demo: Virtual machine with external IP and MetalLB load-balancer</h2>

<p>With the following recipe we will end up with a nginx server running on a virtual machine, accessible outside the cluster using MetalLB load-balancer with Layer 2 mode.</p>

<h3 id="demo-environment-setup">Demo environment setup</h3>

<p>We are going to use <a href="https://kind.sigs.k8s.io">kind</a> provider as an ephemeral Kubernetes cluster.</p>

<p>Prerequirements:</p>
<ul>
  <li>First install kind on your machine following its <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">installation guide</a>.</li>
  <li>To use kind, you will also need to <a href="https://docs.docker.com/install/">install docker</a>.</li>
</ul>

<h4 id="external-ips-on-macos-and-windows">External IPs on macOS and Windows</h4>

<p>This demo runs Docker on Linux, which allows sending traffic directly to the load-balancer‚Äôs external IP if the IP space is within the docker IP space.
On macOS and Windows however, docker does not expose the docker network to the host, rendering the external IP unreachable from other kind nodes. In order to workaround this, one could expose pods and services using extra port mappings as shown in the extra port mappings section of kind‚Äôs <a href="https://kind.sigs.k8s.io/docs/user/configuration#extra-port-mappings">Configuration Guide</a>.</p>

<h3 id="deploying-cluster">Deploying cluster</h3>

<p>To start a kind cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kind create cluster
</code></pre></div></div>

<p>In order to interact with the specific cluster created:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl cluster-info <span class="nt">--context</span> kind-kind
</code></pre></div></div>

<h3 id="installing-components">Installing components</h3>

<h4 id="installing-metallb-on-the-cluster">Installing MetalLB on the cluster</h4>

<p>There are <a href="https://metallb.universe.tf/installation/">many ways</a> to install MetalLB. For the sake of this example, we will install MetalLB via manifests. To do this, follow this <a href="https://metallb.universe.tf/installation/#installation-by-manifest">guide</a>. 
Confirm successful installation by waiting for MetalLB pods to have a status of Running:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pods <span class="nt">-n</span> metallb-system <span class="nt">--watch</span>
</code></pre></div></div>

<h4 id="installing-kubevirt-on-the-cluster">Installing Kubevirt on the cluster</h4>

<p>Following Kubevirt <a href="https://kubevirt.io/user-guide/operations/installation/#installing-kubevirt-on-kubernetes">user guide</a> to install released version v0.51.0</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RELEASE</span><span class="o">=</span>v0.51.0
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-operator.yaml"</span>
kubectl apply <span class="nt">-f</span> <span class="s2">"https://github.com/kubevirt/kubevirt/releases/download/</span><span class="k">${</span><span class="nv">RELEASE</span><span class="k">}</span><span class="s2">/kubevirt-cr.yaml"</span>
kubectl <span class="nt">-n</span> kubevirt <span class="nb">wait </span>kv kubevirt <span class="nt">--timeout</span><span class="o">=</span>360s <span class="nt">--for</span> <span class="nv">condition</span><span class="o">=</span>Available
</code></pre></div></div>

<p>Now we have a Kubernetes cluster with all the pieces to start the Demo.</p>

<h3 id="network-resources-configuration">Network resources configuration</h3>

<h4 id="setting-address-pool-to-be-used-by-the-loadbalancer">Setting Address Pool to be used by the LoadBalancer</h4>

<p>In order to complete the Layer 2 mode configuration, we need to set a range of IP addresses for the LoadBalancer to use.
On Linux we can use the docker kind network (macOS and Windows users see <a href="#external-ips-on-macos-and-windows">External IPs Prerequirement</a>), so by using this command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker network inspect <span class="nt">-f</span> <span class="s1">''</span> kind
</code></pre></div></div>

<p>You should get the subclass you can set the IP range from. The output should contain a cidr such as 172.18.0.0/16.
Using this result we will create the following Layer 2 address pool with 172.18.1.1-172.18.1.16 range:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">metallb-system</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">config</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">config</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="s">address-pools:</span>
    <span class="s">- name: addresspool-sample1</span>
      <span class="s">protocol: layer2</span>
      <span class="s">addresses:</span>
      <span class="s">- 172.18.1.1-172.18.1.16</span>
<span class="s">EOF</span>
</code></pre></div></div>

<h3 id="network-utilization">Network utilization</h3>

<h4 id="spin-up-a-virtual-machine-running-nginx">Spin up a Virtual Machine running Nginx</h4>

<p>Now it‚Äôs time to start-up a virtual machine running nginx using the following yaml.
The virtual machine has a <code class="language-plaintext highlighter-rouge">metallb-service=nginx</code> we created to use when creating the service.</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubevirt.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">VirtualMachine</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">fedora-nginx</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">running</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">domain</span><span class="pi">:</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="na">disks</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
            <span class="pi">-</span> <span class="na">disk</span><span class="pi">:</span>
                <span class="na">bus</span><span class="pi">:</span> <span class="s">virtio</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
          <span class="na">interfaces</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">masquerade</span><span class="pi">:</span> <span class="pi">{}</span>
              <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
        <span class="na">resources</span><span class="pi">:</span>
          <span class="na">requests</span><span class="pi">:</span>
            <span class="na">memory</span><span class="pi">:</span> <span class="s">1024M</span>
      <span class="na">networks</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">default</span>
          <span class="na">pod</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="na">terminationGracePeriodSeconds</span><span class="pi">:</span> <span class="m">0</span>
      <span class="na">volumes</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">containerDisk</span><span class="pi">:</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">kubevirt/fedora-cloud-container-disk-demo</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">containerdisk</span>
        <span class="pi">-</span> <span class="na">cloudInitNoCloud</span><span class="pi">:</span>
            <span class="na">userData</span><span class="pi">:</span> <span class="pi">|-</span>
              <span class="s">#cloud-config</span>
              <span class="s">password: fedora</span>
              <span class="s">chpasswd: { expire: False }</span>
              <span class="s">packages:</span>
                <span class="s">- nginx</span>
              <span class="s">runcmd:</span>
                <span class="s">- [ "systemctl", "enable", "--now", "nginx" ]</span>
          <span class="na">name</span><span class="pi">:</span> <span class="s">cloudinitdisk</span>
<span class="s">EOF</span>
</code></pre></div></div>

<h4 id="expose-the-virtual-machine-with-a-typed-loadbalancer-service">Expose the virtual machine with a typed <code class="language-plaintext highlighter-rouge">LoadBalancer</code> service</h4>

<p>When creating the <code class="language-plaintext highlighter-rouge">LoadBalancer</code> typed service, we need to remember annotating the address-pool we want to use 
<code class="language-plaintext highlighter-rouge">addresspool-sample1</code> and also add the selector <code class="language-plaintext highlighter-rouge">metallb-service: nginx</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">metallb-nginx-svc</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">default</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">metallb.universe.tf/address-pool</span><span class="pi">:</span> <span class="s">addresspool-sample1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">externalTrafficPolicy</span><span class="pi">:</span> <span class="s">Local</span>
  <span class="na">ipFamilies</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">IPv4</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">tcp-5678</span>
      <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">5678</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">metallb-service</span><span class="pi">:</span> <span class="s">nginx</span>
<span class="s">EOF</span>
</code></pre></div></div>

<p>Notice that the service got assigned with an external IP from the range assigned by the address pool:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get service <span class="nt">-n</span> default metallb-nginx-svc
</code></pre></div></div>

<p>Example output:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>          AGE
metallb-nginx-svc   LoadBalancer   10.96.254.136   172.18.1.1    5678:32438/TCP   53s
</code></pre></div></div>

<h4 id="access-the-virtual-machine-from-outside-the-cluster">Access the virtual machine from outside the cluster</h4>

<p>Finally, we can check that the nginx server is accessible from outside the cluster:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-s</span> <span class="nt">-o</span> /dev/null 172.18.1.1:5678 <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"URL exists"</span>
</code></pre></div></div>

<p>Example output:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>URL exists
</code></pre></div></div>
<p>Note that it may take a short while for the URL to work after setting the service.</p>

<h2 id="doing-this-on-your-own-cluster">Doing this on your own cluster</h2>

<p>Moving outside the demo example, one who would like use MetalLB on their real life cluster, should also take other considerations in mind:</p>
<ul>
  <li>User privileges: you should have <code class="language-plaintext highlighter-rouge">cluster-admin</code> privileges on the cluster - in order to install MetalLB.</li>
  <li>IP Ranges for MetalLB: getting IP Address pools allocation for MetalLB depends on your cluster environment:
    <ul>
      <li>If you‚Äôre running a bare-metal cluster in a shared host environment, you need to first reserve this IP Address pool from your hosting provider.</li>
      <li>Alternatively, if you‚Äôre running on a private cluster, you can use one of the private IP Address spaces (a.k.a RFC1918 addresses). Such addresses are free, and work fine as long as you‚Äôre only providing cluster services to your LAN.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog post we used MetalLB to expose a service using an external IP assigned to a virtual machine. 
This illustrates how virtual machine traffic can be load-balanced via a service.</p>]]></content><author><name>Ram Lavi</name></author><category term="news" /><category term="Kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="load-balancer" /><category term="MetalLB" /><summary type="html"><![CDATA[This post illustrates setting up a virtual machine with MetalLB LoadBalancer service.]]></summary></entry><entry><title type="html">KubeVirt v0.51.0</title><link href="https://kubevirt.io//2022/changelog-v0.51.0.html" rel="alternate" type="text/html" title="KubeVirt v0.51.0" /><published>2022-03-08T00:00:00+00:00</published><updated>2022-03-08T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.51.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.51.0.html"><![CDATA[<h2 id="v0510">v0.51.0</h2>

<p>Released on: Tue Mar 8 21:06:59 2022 +0000</p>

<ul>
  <li>[PR #7102][machadovilaca] Add Virtual Machine name label to virt-launcher pod</li>
  <li>[PR #7139][davidvossel] Fixes inconsistent VirtualMachinePool VM/VMI updates by using controller revisions</li>
  <li>[PR #6754][jean-edouard] New and resized disks are now always 1MiB-aligned</li>
  <li>[PR #7086][acardace] Add ‚ÄòEvictionStrategy‚Äô as a cluster-wide setting in the KubeVirt CR</li>
  <li>[PR #7232][rmohr] Properly format the PDB scale event during migrations</li>
  <li>[PR #7223][Barakmor1] Add a name label to virt-operator pods</li>
  <li>[PR #7221][davidvossel] RunStrategy: Once - allows declaring a VM should run once to a finalized state</li>
  <li>[PR #7091][EdDev] SR-IOV interfaces are now reported in the VMI status even without an active guest-agent.</li>
  <li>[PR #7169][rmohr] Improve device plugin de-registration in virt-handler and some test stabilizations</li>
  <li>[PR #6604][alicefr] Add shareable option to identify if the disk is shared with other VMs</li>
  <li>[PR #7144][davidvossel] Garbage collect finalized migration objects only leaving the most recent 5 objects</li>
  <li>[PR #6110][xpivarc] [Nonroot] SRIOV is now available.</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.51.0 changes]]></summary></entry><entry><title type="html">KubeVirt v0.50.0</title><link href="https://kubevirt.io//2022/changelog-v0.50.0.html" rel="alternate" type="text/html" title="KubeVirt v0.50.0" /><published>2022-02-09T00:00:00+00:00</published><updated>2022-02-09T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.50.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.50.0.html"><![CDATA[<h2 id="v0500">v0.50.0</h2>

<p>Released on: Wed Feb 9 18:01:08 2022 +0000</p>

<ul>
  <li>[PR #7056][fossedihelm] Update k8s dependencies to 0.23.1</li>
  <li>[PR #7135][davidvossel] Switch from reflects.DeepEquals to equality.Semantic.DeepEquals() across the entire project</li>
  <li>[PR #7052][sradco] Updated recording rule ‚Äúkubevirt_vm_container_free_memory_bytes‚Äù</li>
  <li>[PR #7000][iholder-redhat] Adds a possibility to override default libvirt log filters though VMI annotations</li>
  <li>[PR #7064][davidvossel] Fixes issue associated with blocked uninstalls when VMIs exist during removal</li>
  <li>[PR #7097][iholder-redhat] [Bug fix] VMI with kernel boot stuck on ‚ÄúTerminating‚Äù status if more disks are defined</li>
  <li>[PR #6700][VirrageS] Simplify replacing <code class="language-plaintext highlighter-rouge">time.Ticker</code> in agent poller and fix default values for <code class="language-plaintext highlighter-rouge">qemu-*-interval</code> flags</li>
  <li>[PR #6581][ormergi] SRIOV network interfaces are now hot-plugged when disconnected manually or due to aborted migrations.</li>
  <li>[PR #6924][EdDev] Support for legacy GPU definition is removed. Please see https://kubevirt.io/user-guide/virtual_machines/host-devices on how to define host-devices.</li>
  <li>[PR #6735][uril] The command <code class="language-plaintext highlighter-rouge">migrate_cancel</code> was added to virtctl. It cancels an active VM migration.</li>
  <li>[PR #6883][rthallisey] Add instance-type to cloud-init metadata</li>
  <li>[PR #6999][maya-r] When expanding disk images, take the minimum between the request and the capacity - avoid using the full underlying file system on storage like NFS, local.</li>
  <li>[PR #6946][vladikr] Numa information of an assigned device will be presented in the devices metadata</li>
  <li>[PR #6042][iholder-redhat] Fully support cgroups v2, include a new cohesive package and perform major refactoring.</li>
  <li>[PR #6968][vladikr] Added Writeback disk cache support</li>
  <li>[PR #6995][sradco] Alert OrphanedVirtualMachineImages name was changed to OrphanedVirtualMachineInstances.</li>
  <li>[PR #6923][rhrazdil] Fix issue with ssh being unreachable on VMIs with Istio proxy</li>
  <li>[PR #6821][jean-edouard] Migrating VMIs that contain dedicated CPUs will now have properly dedicated CPUs on target</li>
  <li>[PR #6793][oshoval] Add infoSource field to vmi.status.interfaces.</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.50.0 changes]]></summary></entry><entry><title type="html">Dedicated migration network in KubeVirt</title><link href="https://kubevirt.io//2022/Dedicated-migration-network.html" rel="alternate" type="text/html" title="Dedicated migration network in KubeVirt" /><published>2022-01-25T00:00:00+00:00</published><updated>2022-01-25T00:00:00+00:00</updated><id>https://kubevirt.io//2022/Dedicated-migration-network</id><content type="html" xml:base="https://kubevirt.io//2022/Dedicated-migration-network.html"><![CDATA[<p>Since version 0.49, KubeVirt supports live migrating VMIs over a separate network than the one Kubernetes is running on.</p>

<p>Running migrations over a dedicated network is a great way to increase migration bandwidth and reliability.</p>

<p>This article gives an overview of the feature as well as a concrete example. For more technical information, refer to the <a href="https://kubevirt.io/user-guide/operations/live_migration/#using-a-different-network-for-migrations">KubeVirt documentation</a>.</p>

<h2 id="hardware-configuration">Hardware configuration</h2>

<p>The simplest way to use the feature is to find an unused NIC on every worker node, and to connect them all to the same switch.</p>

<p>All NICs must have the same name. If they don‚Äôt, they should be permanently renamed.
The process for renaming NICs varies depending on your operating system, refer to its documentation if you need help.</p>

<p>Adding servers to the network for services like DHCP or DNS is an option but it is not required.
If a DHCP is running, it is best if it doesn‚Äôt provide routes to other networks / the internet, to keep the migration network isolated.</p>

<h2 id="cluster-configuration">Cluster configuration</h2>

<p>The interface between the physical network and KubeVirt is a NetworkAttachmentDefinition (NAD), created in the namespace where KubeVirt is installed.</p>

<p>The implementation of the NAD is up to the admin, as long as it provides a link to the secondary network.
The admin must also ensure that the NAD is able to provide cluster-wide IPs, either through a physical DHCP, or with another CNI plugin like <a href="https://github.com/k8snetworkplumbingwg/whereabouts">whereabouts</a></p>

<p>Important: the subnet used here must be completely distinct from the ones used by the main Kubernetes network, to ensure proper routing.</p>

<h2 id="testing">Testing</h2>

<p>If you just want to test the feature, KubeVirtCI supports the creation of multiple nodes, as well as secondary networks.
All you need is to define the right environment variables before starting the cluster.</p>

<p>See the example below for more info (note that text in the ‚Äúvideo‚Äù can actually be selected and copy/pasted).</p>

<h2 id="example">Example</h2>

<p>Here is a quick <a href="https://asciinema.org/a/464272">example</a> of a dual-node KubeVirtCI cluster running a migration over a secondary network.</p>

<p>The description of the clip includes more detailed information about the steps involved.</p>]]></content><author><name>Jed Lejosne</name></author><category term="news" /><category term="kubevirt" /><category term="kubernetes" /><category term="virtual machine" /><category term="VM" /><category term="live migration" /><category term="dedicated network" /><summary type="html"><![CDATA[KubeVirt now supports using a separate network for live migrations]]></summary></entry><entry><title type="html">KubeVirt Summit is coming back!</title><link href="https://kubevirt.io//2022/KubeVirt-Summit-2022.html" rel="alternate" type="text/html" title="KubeVirt Summit is coming back!" /><published>2022-01-24T00:00:00+00:00</published><updated>2022-01-24T00:00:00+00:00</updated><id>https://kubevirt.io//2022/KubeVirt-Summit-2022</id><content type="html" xml:base="https://kubevirt.io//2022/KubeVirt-Summit-2022.html"><![CDATA[<p>The second online <a href="/summit/">KubeVirt Summit</a> is coming on February 16, 2022!</p>

<h2 id="when">When</h2>

<p>The event will take place online during two half-days:</p>

<ul>
  <li>Dates: February 16 and 17, 2022.</li>
  <li>Time: 14:00 ‚Äì 19:00 UTC (9:00‚Äì14:00 EST, 15:00‚Äì20:00 CET)</li>
</ul>

<h2 id="register">Register</h2>

<p><a href="/summit/">KubeVirt Summit</a> is hosted on Community.CNCF.io. Because of how that platform works, you need to register for each of the two days of the summit independantly:</p>

<ul>
  <li><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2022-day-1/">Register for Day 1</a></li>
  <li><a href="https://community.cncf.io/events/details/cncf-kubevirt-community-presents-kubevirt-summit-2022-day-2/">Register for Day 2</a></li>
</ul>

<p>You will need to create an account with CNCF.io if you have not before. Attendance is free.</p>

<h2 id="keep-up-to-date">Keep up to date</h2>

<p>Connect with the KubeVirt Community through our <a href="/community">community page</a>.</p>

<p>We are looking forward to meeting you there!</p>]]></content><author><name>Chandler Wilkerson</name></author><category term="news" /><category term="kubevirt" /><category term="event" /><category term="community" /><summary type="html"><![CDATA[Join us for the KubeVirt community's second annual dedicated online event]]></summary></entry><entry><title type="html">KubeVirt v0.49.0</title><link href="https://kubevirt.io//2022/changelog-v0.49.0.html" rel="alternate" type="text/html" title="KubeVirt v0.49.0" /><published>2022-01-11T00:00:00+00:00</published><updated>2022-01-11T00:00:00+00:00</updated><id>https://kubevirt.io//2022/changelog-v0.49.0</id><content type="html" xml:base="https://kubevirt.io//2022/changelog-v0.49.0.html"><![CDATA[<h2 id="v0490">v0.49.0</h2>

<p>Released on: Tue Jan 11 17:27:09 2022 +0000</p>

<ul>
  <li>[PR #7004][iholder-redhat] Bugfix: Avoid setting block migration for volumes used by read-only disks</li>
  <li>[PR #6959][enp0s3] generate event when target pod enters unschedulable phase</li>
  <li>[PR #6888][assafad] Added common labels into alert definitions</li>
  <li>[PR #6166][vasiliy-ul] Experimental support of AMD SEV</li>
  <li>[PR #6980][vasiliy-ul] Updated the dependencies to include the fix for CVE-2021-43565 (KubeVirt is not affected)</li>
  <li>[PR #6944][iholder-redhat] Remove disabling TLS configuration from Live Migration Policies</li>
  <li>[PR #6800][jean-edouard] CPU pinning doesn‚Äôt require hardware-assisted virtualization anymore</li>
  <li>[PR #6501][ShellyKa13] Use virtctl image-upload to upload archive content</li>
  <li>[PR #6918][iholder-redhat] Bug fix: Unscheduable host-model VMI alert is now properly triggered</li>
  <li>[PR #6796][Barakmor1] ‚Äòkubevirt-operator‚Äô changed to ‚Äòvirt-operator‚Äô on ‚Äòmanaged-by‚Äô label in kubevirt‚Äôs components made by virt-operator</li>
  <li>[PR #6036][jean-edouard] Migrations can now be done over a dedicated multus network</li>
  <li>[PR #6933][erkanerol] Add a new lane for monitoring tests</li>
  <li>[PR #6949][jean-edouard] KubeVirt components should now be successfully removed on CR deletion, even when using only 1 replica for virt-api and virt-controller</li>
  <li>[PR #6954][maiqueb] Update the <code class="language-plaintext highlighter-rouge">virtctl</code> exposed services <code class="language-plaintext highlighter-rouge">IPFamilyPolicyType</code> default to <code class="language-plaintext highlighter-rouge">IPFamilyPolicyPreferDualStack</code></li>
  <li>[PR #6931][fossedihelm] added DryRun to AddVolumeOptions and RemoveVolumeOptions</li>
  <li>[PR #6379][nunnatsa] Fix issue https://bugzilla.redhat.com/show_bug.cgi?id=1945593</li>
  <li>[PR #6399][iholder-redhat] Introduce live migration policies that allow system-admins to have fine-grained control over migration configuration for different sets of VMs.</li>
  <li>[PR #6880][iholder-redhat] Add full Podman support for <code class="language-plaintext highlighter-rouge">make</code> and <code class="language-plaintext highlighter-rouge">make test</code></li>
  <li>[PR #6702][acardace] implement virt-handler canary upgrade and rollback for faster and safer rollouts</li>
  <li>[PR #6717][davidvossel] Introducing the VirtualMachinePools feature for managing stateful VMs at scale</li>
  <li>[PR #6698][rthallisey] Add tracing to the virt-controller work queue</li>
  <li>[PR #6762][fossedihelm] added DryRun mode to virtcl to migrate command</li>
  <li>[PR #6891][rmohr] Fix ‚ÄúMake raw terminal failed: The handle is invalid?‚Äù issue with ‚Äúvirtctl console‚Äù when not executed in a pty</li>
  <li>[PR #6783][rmohr] Skip SSH RSA auth if no RSA key was explicitly provided and not key exists at the default location</li>
</ul>]]></content><author><name>kubeü§ñ</name></author><category term="releases" /><category term="release notes" /><category term="changelog" /><summary type="html"><![CDATA[This article provides information about KubeVirt release v0.49.0 changes]]></summary></entry></feed>